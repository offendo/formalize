base_model: "/volume/autoformalization/models/full_herald_iter3_positives/checkpoint-best"

# Performance optimizations
# =========================
plugins:
  # - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin
  - axolotl.integrations.liger.LigerPlugin

## Deepspeed
deepspeed: deepspeed_configs/zero1.json

## CCE
cut_cross_entropy: false # doesn't work with qwen

## Liger
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true

# Training settings
# =================
## Training
micro_batch_size: 4
auto_find_batch_size: true
gradient_accumulation_steps: 2
num_epochs: 3
learning_rate: 0.0001
warmup_ratio: 0.05
lr_scheduler: cosine
optimizer: paged_adamw_8bit
output_dir: /volume/autoformalization/models/full_herald_iter4_positives/

## Logging and saving
logging_steps: 10
eval_strategy: "no" # Set to `"no"` to skip evaluation, `"epoch"` at end of each epoch, leave empty to infer from `eval_steps`.
save_strategy: # Set to `"no"` to skip checkpoint saves, `"epoch"` at end of each epoch, `"best"` when better result is achieved, leave empty to infer from `save_steps`.
save_steps: 100 # Leave empty to save at each epoch, integer for every N steps. float for fraction of total steps
save_total_limit: 1 # Checkpoints saved at a time
wandb_project: herald-em-autoformalization # Your wandb project name
wandb_name: full_herald_iter4_positives # Set the name of your wandb run

## Performance
bf16: true
flash_attention: false
gradient_checkpointing: true

# Dataset
# =======
hf_use_auth_token: true
sequence_len: 2048
dataset_prepared_path: dataset
dataset_processes: 1
datasets:
  - path: offendo/herald_iter3_positives_filtered
    type: chat_template
    roles_to_train: ["assistant"]
    chat_template: tokenizer_default
    field_messages: conversation
